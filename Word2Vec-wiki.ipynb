{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Load pre-trained Word2Vec model.\n",
    "model = gensim.models.Word2Vec.load(\"300features-2window-40mincount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"wiki_texts_tokenized.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text=train.readlines()\n",
    "print len(train_text)\n",
    "#print train_text[0]\n",
    "##Seen lots of noises in the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Remove repeating Pound Sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=[re.sub('#*#','#',line) for line in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print train_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove English Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=[re.sub('#[a-zA-Z]* ','',line) for line in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print train_text[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = bs(train[\"review\"][0])\n",
    "print example1.get_text() #Remove all tabs and markups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_only = re.sub(\"[^a-zA-Z]\",\" \",example1.get_text() )  # The text to search\n",
    "print letters_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Uniform lower and upper cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = letters_only.lower()        # Convert to lower case\n",
    "words = lower_case.split()               # Split into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dic = {x:words.count(x) for x in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print stopwords.words(\"english\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def article_to_sentences(article, tokenizer, remove_stopwords=False):\n",
    "    sentences=[]\n",
    "    rev=bs(article.strip()).get_text()\n",
    "    letter_only=re.sub(\"[^a-zA-Z]\",\" \",rev)\n",
    "    lower_case=letter_only.lower()\n",
    "    raw_sentences=tokenizer.tokenize(lower_case)\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence)>1:\n",
    "            words=sentence.split()\n",
    "            if remove_stopwords:\n",
    "                words=[x for x in words if x not in stopwords.words('english')]\n",
    "            sentences.append(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-11-30 12:44:52,348 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide sentences by tokenizer\n",
    "def divide_sentences_review(train,tokenizer):\n",
    "    sentences=[]\n",
    "    num_reviews=len(train['review'])\n",
    "    print 'divide reviews into sentences' #word2vec requires input to be a set of sentences as lits of words.\n",
    "    for i in range(num_reviews):\n",
    "        sentences += article_to_sentences(train['review'][i],tokenizer)\n",
    "        if((i+1)%1000 == 0):\n",
    "            print \"Review %d th\" % (i+1)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_sentences(train):\n",
    "    sentences=[]\n",
    "    num_articles=len(train)\n",
    "    print 'divide articles into sentences...' #word2vec requires input to be a set of sentences as lits of words.\n",
    "    for i in range(num_articles):\n",
    "        for sentence in train[i].split('#'):\n",
    "            if len(sentence)>1:\n",
    "                sentences.append(sentence.split())\n",
    "        if((i+1)%1000 == 0):\n",
    "            print \"Review %d th\" % (i+1)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(sentences,window_size,model_name):\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(sentences, workers=4, \\\n",
    "                size=300, min_count = 10, \\\n",
    "                window = window_size, sample = 0.003)\n",
    "    model.save(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences=divide_sentences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=train_model(sentences,2,'300features-2window-40mincount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_output=open('wordlist.txt','w')\n",
    "word_output.write('\\n'.join(model.wv.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model.wv.vocab.keys()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平埔族,0.764345\n",
      "泰雅族,0.742261\n",
      "住民,0.735763\n",
      "土着,0.714229\n",
      "西拉雅,0.710930\n",
      "邹族,0.699622\n",
      "阿美族,0.688832\n",
      "原住,0.677996\n",
      "布农族,0.677511\n",
      "汉人,0.671237\n"
     ]
    }
   ],
   "source": [
    "for word,sim in model.most_similar('原住民'): #天安门，毛泽东，江泽民，习近平，曾志伟，中国，中华民国，劳动教养，警察，文革\n",
    "    print '%s,%f' % (word,sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model.doesnt_match(\"中国 台湾地区 中华民国 中华人民共和国 我国\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model.wv.most_similar(positive=['毛泽东', '反对'], negative=['邓小平'])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab['阿基米德'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.syn0[model.wv.vocab['中国'].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score([\"I don't like this movie\".split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "phrases_2 = gensim.models.Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(phrases_2[sentences[0:2]])\n",
    "print test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_3= gensim.models.Phrases(phrases_2[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=list(phrases_3[test])\n",
    "print test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "laojiao = open(\"weibo_2016_tokens.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_weight=pd.DataFrame(model.wv.syn0norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警察\n"
     ]
    }
   ],
   "source": [
    "print model.wv.index2word[1079]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting -- Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num=len(model.wv.vocab.keys())\n",
    "laojiao_vec=[]\n",
    "for i in range(len(laojiao)):\n",
    "    case_vec=[0]*num\n",
    "    line=laojiao[i].strip()\n",
    "    words=line.split()\n",
    "    length=len(words)\n",
    "    for word in set(words):\n",
    "        tf=words.count(word) #absolute word freq\n",
    "        tf=round(float(tf)/length,2) #relative word freq\n",
    "        if word in model.wv.vocab.keys():\n",
    "            case_vec[model.wv.vocab[word].index]=tf\n",
    "    case_vec=pd.Series(case_vec)\n",
    "    output_vec=case_vec.dot(word2vec_weight).tolist()\n",
    "    output_vec=[round(j,2) for j in output_vec]\n",
    "    laojiao_vec.append(output_vec)\n",
    "    if i % 100 ==0:\n",
    "        print \"%d Lines Have Been Vectorized\" % i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting -- Down weighting frequent words (Pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total=0\n",
    "for word in model.wv.vocab.keys():\n",
    "    Total+=model.wv.vocab[word].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_num=len(model.wv.vocab.keys())\n",
    "laojiao_vec=[]\n",
    "a=0.001\n",
    "iter_num=len(laojiao)\n",
    "for i in range(iter_num):\n",
    "    case_vec=[0]*vocab_num\n",
    "    line=laojiao[i].strip()\n",
    "    words=line.split()\n",
    "    for word in set(words):\n",
    "        tf=float(\"{0:.1f}\".format(words.count(word))) #absolute word freq in doc\n",
    "        if word in model.wv.vocab.keys():\n",
    "            word_freq=model.wv.vocab[word].count/float(Total)\n",
    "            case_vec[model.wv.vocab[word].index]=a/(a+tf*word_freq)\n",
    "    case_vec=pd.Series(case_vec)\n",
    "    output_vec=case_vec.dot(word2vec_weight).tolist()\n",
    "    output_vec=[round(j,2) for j in output_vec]\n",
    "    laojiao_vec.append(output_vec)\n",
    "    if i % 100 ==0:\n",
    "        print \"%d Lines Have Been Vectorized\" % i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting -- Down weighting frequent words (Testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup dense matrix of vocab\n",
      "0 Lines Have Been Vectorized\n",
      "100 Lines Have Been Vectorized\n",
      "200 Lines Have Been Vectorized\n",
      "300 Lines Have Been Vectorized\n",
      "400 Lines Have Been Vectorized\n",
      "500 Lines Have Been Vectorized\n",
      "600 Lines Have Been Vectorized\n",
      "700 Lines Have Been Vectorized\n",
      "800 Lines Have Been Vectorized\n",
      "900 Lines Have Been Vectorized\n",
      "1000 Lines Have Been Vectorized\n",
      "1100 Lines Have Been Vectorized\n",
      "1200 Lines Have Been Vectorized\n",
      "1300 Lines Have Been Vectorized\n",
      "1400 Lines Have Been Vectorized\n",
      "1500 Lines Have Been Vectorized\n",
      "1600 Lines Have Been Vectorized\n",
      "1700 Lines Have Been Vectorized\n",
      "1800 Lines Have Been Vectorized\n",
      "1900 Lines Have Been Vectorized\n",
      "2000 Lines Have Been Vectorized\n",
      "2100 Lines Have Been Vectorized\n",
      "2200 Lines Have Been Vectorized\n",
      "2300 Lines Have Been Vectorized\n",
      "2400 Lines Have Been Vectorized\n",
      "2500 Lines Have Been Vectorized\n",
      "2600 Lines Have Been Vectorized\n",
      "2700 Lines Have Been Vectorized\n",
      "2800 Lines Have Been Vectorized\n",
      "2900 Lines Have Been Vectorized\n",
      "3000 Lines Have Been Vectorized\n",
      "3100 Lines Have Been Vectorized\n",
      "3200 Lines Have Been Vectorized\n",
      "3300 Lines Have Been Vectorized\n",
      "3400 Lines Have Been Vectorized\n",
      "3500 Lines Have Been Vectorized\n",
      "3600 Lines Have Been Vectorized\n",
      "3700 Lines Have Been Vectorized\n",
      "3800 Lines Have Been Vectorized\n",
      "3900 Lines Have Been Vectorized\n",
      "4000 Lines Have Been Vectorized\n",
      "4100 Lines Have Been Vectorized\n",
      "4200 Lines Have Been Vectorized\n",
      "4300 Lines Have Been Vectorized\n",
      "4400 Lines Have Been Vectorized\n",
      "4500 Lines Have Been Vectorized\n",
      "4600 Lines Have Been Vectorized\n",
      "4700 Lines Have Been Vectorized\n",
      "4800 Lines Have Been Vectorized\n",
      "4900 Lines Have Been Vectorized\n",
      "5000 Lines Have Been Vectorized\n",
      "5100 Lines Have Been Vectorized\n",
      "5200 Lines Have Been Vectorized\n",
      "5300 Lines Have Been Vectorized\n",
      "5400 Lines Have Been Vectorized\n",
      "5500 Lines Have Been Vectorized\n",
      "5600 Lines Have Been Vectorized\n",
      "5700 Lines Have Been Vectorized\n",
      "5800 Lines Have Been Vectorized\n",
      "5900 Lines Have Been Vectorized\n",
      "6000 Lines Have Been Vectorized\n",
      "6100 Lines Have Been Vectorized\n",
      "6200 Lines Have Been Vectorized\n",
      "6300 Lines Have Been Vectorized\n",
      "6400 Lines Have Been Vectorized\n",
      "6500 Lines Have Been Vectorized\n",
      "6600 Lines Have Been Vectorized\n",
      "6700 Lines Have Been Vectorized\n",
      "6800 Lines Have Been Vectorized\n",
      "6900 Lines Have Been Vectorized\n",
      "7000 Lines Have Been Vectorized\n",
      "7100 Lines Have Been Vectorized\n",
      "7200 Lines Have Been Vectorized\n",
      "7300 Lines Have Been Vectorized\n",
      "7400 Lines Have Been Vectorized\n",
      "7500 Lines Have Been Vectorized\n",
      "7600 Lines Have Been Vectorized\n",
      "7700 Lines Have Been Vectorized\n",
      "7800 Lines Have Been Vectorized\n",
      "7900 Lines Have Been Vectorized\n",
      "8000 Lines Have Been Vectorized\n",
      "8100 Lines Have Been Vectorized\n",
      "8200 Lines Have Been Vectorized\n",
      "8300 Lines Have Been Vectorized\n",
      "8400 Lines Have Been Vectorized\n",
      "8500 Lines Have Been Vectorized\n",
      "8600 Lines Have Been Vectorized\n",
      "8700 Lines Have Been Vectorized\n",
      "8800 Lines Have Been Vectorized\n",
      "8900 Lines Have Been Vectorized\n",
      "9000 Lines Have Been Vectorized\n",
      "9100 Lines Have Been Vectorized\n",
      "9200 Lines Have Been Vectorized\n",
      "9300 Lines Have Been Vectorized\n",
      "9400 Lines Have Been Vectorized\n",
      "9500 Lines Have Been Vectorized\n",
      "9600 Lines Have Been Vectorized\n",
      "9700 Lines Have Been Vectorized\n",
      "9800 Lines Have Been Vectorized\n",
      "9900 Lines Have Been Vectorized\n"
     ]
    }
   ],
   "source": [
    "#Use dense matrix representation to save storage\n",
    "vocab_num=len(model.wv.vocab.keys())\n",
    "dense_vec=[]\n",
    "iter_num=len(laojiao)\n",
    "\n",
    "print \"Setup dense matrix of vocab\"\n",
    "for i in range(iter_num):\n",
    "    line_vec=[]\n",
    "    case_vec=[0]*vocab_num\n",
    "    line=laojiao[i].strip()\n",
    "    words=line.split()\n",
    "    for word in words:\n",
    "        if word in model.wv.vocab.keys():\n",
    "            line_vec.append(model.wv.vocab[word].index)\n",
    "    dense_vec.append(line_vec)\n",
    "    if i % 100 ==0:\n",
    "        print \"%d Lines Have Been Vectorized\" % i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate frequencies of vocab in whole Test-set\n",
      "Done! Have updated 16983 Words' Frequencies.\n"
     ]
    }
   ],
   "source": [
    "print \"Calculate frequencies of vocab in whole Test-set\"\n",
    "dic={}\n",
    "flat_list=[i for j in dense_vec for i in j]\n",
    "Total=len(flat_list)\n",
    "for word in set(flat_list):\n",
    "    dic[word]=flat_list.count(word)/float(Total)\n",
    "\n",
    "print \"Done! Have updated %d Words' Frequencies.\" % len(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16983"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7581"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国 0.00256\n",
      "一个 0.01044\n",
      "香港 0.00057\n",
      "台湾 0.00016\n",
      "日本 0.00044\n",
      "美国 0.00072\n",
      "开始 0.00107\n",
      "可以 0.00604\n",
      "使用 0.00056\n",
      "成为 0.00069\n"
     ]
    }
   ],
   "source": [
    "for k in dic.keys()[0:10]:\n",
    "    print model.wv.index2word[k],round(dic[k],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Lines Have Been Vectorized\n",
      "200 Lines Have Been Vectorized\n",
      "300 Lines Have Been Vectorized\n",
      "400 Lines Have Been Vectorized\n",
      "500 Lines Have Been Vectorized\n",
      "600 Lines Have Been Vectorized\n",
      "700 Lines Have Been Vectorized\n",
      "800 Lines Have Been Vectorized\n",
      "900 Lines Have Been Vectorized\n",
      "1000 Lines Have Been Vectorized\n",
      "1100 Lines Have Been Vectorized\n",
      "1200 Lines Have Been Vectorized\n",
      "1300 Lines Have Been Vectorized\n",
      "1400 Lines Have Been Vectorized\n",
      "1500 Lines Have Been Vectorized\n",
      "1600 Lines Have Been Vectorized\n",
      "1700 Lines Have Been Vectorized\n",
      "1800 Lines Have Been Vectorized\n",
      "1900 Lines Have Been Vectorized\n",
      "2000 Lines Have Been Vectorized\n",
      "2100 Lines Have Been Vectorized\n",
      "2200 Lines Have Been Vectorized\n",
      "2300 Lines Have Been Vectorized\n",
      "2400 Lines Have Been Vectorized\n",
      "2500 Lines Have Been Vectorized\n",
      "2600 Lines Have Been Vectorized\n",
      "2700 Lines Have Been Vectorized\n",
      "2800 Lines Have Been Vectorized\n",
      "2900 Lines Have Been Vectorized\n",
      "3000 Lines Have Been Vectorized\n",
      "3100 Lines Have Been Vectorized\n",
      "3200 Lines Have Been Vectorized\n",
      "3300 Lines Have Been Vectorized\n",
      "3400 Lines Have Been Vectorized\n",
      "3500 Lines Have Been Vectorized\n",
      "3600 Lines Have Been Vectorized\n",
      "3700 Lines Have Been Vectorized\n",
      "3800 Lines Have Been Vectorized\n",
      "3900 Lines Have Been Vectorized\n",
      "4000 Lines Have Been Vectorized\n",
      "4100 Lines Have Been Vectorized\n",
      "4200 Lines Have Been Vectorized\n",
      "4300 Lines Have Been Vectorized\n",
      "4400 Lines Have Been Vectorized\n",
      "4500 Lines Have Been Vectorized\n",
      "4600 Lines Have Been Vectorized\n",
      "4700 Lines Have Been Vectorized\n",
      "4800 Lines Have Been Vectorized\n",
      "4900 Lines Have Been Vectorized\n",
      "5000 Lines Have Been Vectorized\n",
      "5100 Lines Have Been Vectorized\n",
      "5200 Lines Have Been Vectorized\n",
      "5300 Lines Have Been Vectorized\n",
      "5400 Lines Have Been Vectorized\n",
      "5500 Lines Have Been Vectorized\n",
      "5600 Lines Have Been Vectorized\n",
      "5700 Lines Have Been Vectorized\n",
      "5800 Lines Have Been Vectorized\n",
      "5900 Lines Have Been Vectorized\n",
      "6000 Lines Have Been Vectorized\n",
      "6100 Lines Have Been Vectorized\n",
      "6200 Lines Have Been Vectorized\n",
      "6300 Lines Have Been Vectorized\n",
      "6400 Lines Have Been Vectorized\n",
      "6500 Lines Have Been Vectorized\n",
      "6600 Lines Have Been Vectorized\n",
      "6700 Lines Have Been Vectorized\n",
      "6800 Lines Have Been Vectorized\n",
      "6900 Lines Have Been Vectorized\n",
      "7000 Lines Have Been Vectorized\n",
      "7100 Lines Have Been Vectorized\n",
      "7200 Lines Have Been Vectorized\n",
      "7300 Lines Have Been Vectorized\n",
      "7400 Lines Have Been Vectorized\n",
      "7500 Lines Have Been Vectorized\n",
      "7600 Lines Have Been Vectorized\n",
      "7700 Lines Have Been Vectorized\n",
      "7800 Lines Have Been Vectorized\n",
      "7900 Lines Have Been Vectorized\n",
      "8000 Lines Have Been Vectorized\n",
      "8100 Lines Have Been Vectorized\n",
      "8200 Lines Have Been Vectorized\n",
      "8300 Lines Have Been Vectorized\n",
      "8400 Lines Have Been Vectorized\n",
      "8500 Lines Have Been Vectorized\n",
      "8600 Lines Have Been Vectorized\n",
      "8700 Lines Have Been Vectorized\n",
      "8800 Lines Have Been Vectorized\n",
      "8900 Lines Have Been Vectorized\n",
      "9000 Lines Have Been Vectorized\n",
      "9100 Lines Have Been Vectorized\n",
      "9200 Lines Have Been Vectorized\n",
      "9300 Lines Have Been Vectorized\n",
      "9400 Lines Have Been Vectorized\n",
      "9500 Lines Have Been Vectorized\n",
      "9600 Lines Have Been Vectorized\n",
      "9700 Lines Have Been Vectorized\n",
      "9800 Lines Have Been Vectorized\n",
      "9900 Lines Have Been Vectorized\n",
      "10000 Lines Have Been Vectorized\n"
     ]
    }
   ],
   "source": [
    "vocab_num=len(model.wv.vocab.keys())\n",
    "word2vec_weight=pd.DataFrame(model.wv.syn0)\n",
    "laojiao_vec=[]\n",
    "a=0.001\n",
    "iter_num=len(laojiao)\n",
    "k=1\n",
    "for case in dense_vec:\n",
    "    case_vec=[0]*vocab_num\n",
    "    for word in set(case):\n",
    "        tf=float(\"{0:.1f}\".format(case.count(word))) #absolute word freq in doc\n",
    "        word_freq=dic[word]\n",
    "        case_vec[int(word)]=tf*a/(a+word_freq)\n",
    "    case_vec=pd.Series(case_vec)\n",
    "    output_vec=case_vec.dot(word2vec_weight).tolist()\n",
    "    output_vec=[round(j,2) for j in output_vec]\n",
    "    laojiao_vec.append(output_vec)\n",
    "    if k % 100 ==0:\n",
    "        print \"%d Lines Have Been Vectorized\" % k\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (features reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(laojiao_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86835468807682525"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "laojiao_svd=pca.transform(laojiao_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12863"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(laojiao_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "laojiao_svd=[i[1:] for i in laojiao_svd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as sim\n",
    "pairwise_similarity=sim(laojiao_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=2\n",
    "pairwise_similarity[n][n+1:].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.77744923 -0.17128495  1.99389749 -1.17916814 -0.94547904  1.12345079\n",
      "  0.08958934 -0.5698971  -0.27551292 -0.24020523  0.71446441 -1.66644431\n",
      " -1.52628525 -0.32312708 -0.10529647 -0.9919811  -1.12826179 -0.00907144\n",
      "  0.17191859]\n",
      "[ 0.77744923 -0.17128495  1.99389749 -1.17916814 -0.94547904  1.12345079\n",
      "  0.08958934 -0.5698971  -0.27551292 -0.24020523  0.71446441 -1.66644431\n",
      " -1.52628525 -0.32312708 -0.10529647 -0.9919811  -1.12826179 -0.00907144\n",
      "  0.17191859]\n"
     ]
    }
   ],
   "source": [
    "j+=1\n",
    "k=pairwise_similarity[j].argsort()[-2]\n",
    "print laojiao_svd[j]\n",
    "print laojiao_svd[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哥们 小伙伴 惊呆 \n",
      "\n",
      "哥们 小伙伴 惊呆 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n+=1\n",
    "print laojiao[n]\n",
    "print laojiao[pairwise_similarity[n].argsort()[-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans.fit(laojiao_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7581"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(laojiao_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5060\n",
      "1 811\n",
      "2 166\n",
      "3 69\n",
      "4 136\n",
      "5 167\n",
      "6 2772\n",
      "7 414\n",
      "8 290\n",
      "9 115\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print i,len(numpy.array(laojiao)[kmeans.labels_==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "太强大 穿透力 花心 给力 \n",
      "\n",
      "哈哈 太有 整人 大师 给力 威武 来看 看吧 \n",
      "\n",
      "特等奖 #崔晓静 1234 哈哈哈哈 哈哈 \n",
      "\n",
      "奇葩 夫妻 欢乐 节操 一地 分享 大家 赶紧 围观 哈哈 \n",
      "\n",
      "#络上 男女 #介绍 #微博 #哥靓 ##信上 放电 抛媚 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in range(5):\n",
    "    boolean=kmeans.labels_==j\n",
    "    l=len(numpy.array(laojiao)[boolean])\n",
    "    print numpy.array(laojiao)[boolean][numpy.random.randint(l)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
