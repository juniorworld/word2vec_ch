{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf-8')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def initiate():\n",
    "\tDocuments=[]\n",
    "\tStoplist=[]\n",
    "\t#initiate doc\n",
    "\tf=open('./phc.txt')\n",
    "\tfor line in f.readlines():\n",
    "\t\tline=line.strip()\n",
    "\t\tline=line.strip(\"'\")\n",
    "\t\tDocuments.append(line)\n",
    "\t#initiate stop lists\n",
    "\tf2=open('./stopwords.txt')\n",
    "\tfor line in f2.readlines():\n",
    "\t\tline=line.strip('\\n')\n",
    "\t\tStoplist.append(line)\n",
    "\treturn Documents,Stoplist\n",
    "\n",
    "def BagOfWord(Documents,Stoplist):\n",
    "\t#tokenize\n",
    "\tTexts = [[word for word in document.lower().split() if word not in Stoplist] for document in Documents]\n",
    "\t#initiate keyword lists\n",
    "\tFrequency={}\n",
    "\tfor line in Texts:\n",
    "\t\tn=0\n",
    "\t\tfor word in line:\n",
    "\t\t\tFrequency[word]=Frequency.get(word,0)+1\n",
    "\t#delete words that only appear once\n",
    "\tfor key in Frequency.keys():\n",
    "\t\tif Frequency[key]==1 or key[0:4]=='http':\n",
    "\t\t\tdel Frequency[key]\n",
    "\t#update Texts\n",
    "\tTexts=[[word for word in document if word in Frequency.keys()] for document in Texts]\n",
    "\t#build corpus\n",
    "\tdictionary = corpora.Dictionary(Texts)\n",
    "\tdictionary.save('./AMR-words.dict')\n",
    "\tcorpus = [dictionary.doc2bow(text) for text in Texts]\n",
    "\tcorpora.MmCorpus.serialize('./AMR-corpus.mm', corpus)\n",
    "\tprint(dictionary.token2id)\n",
    "\treturn dictionary,corpus\n",
    "\n",
    "def output(corpus,dictionary,lda):\n",
    "\tglobal topicnum\n",
    "\t#print doc-topic\n",
    "\tf3=open('./AMR-doc-topic.txt','w')\n",
    "\tn=0\n",
    "\tfor line in corpus:\n",
    "\t\ttopiclist=lda.get_document_topics(line)\n",
    "\t\tcorpus[n].append(topiclist[0][0])\n",
    "\t\tf3.write('%d,%d,%f\\n'%(n,topiclist[0][0],topiclist[0][1]))\n",
    "\t\tn=n+1\n",
    "\tf3.close\n",
    "\t#print topic-words\n",
    "\tf4=open('./AMR-topic-words.txt','w')\n",
    "\tword2id=dictionary.token2id\n",
    "\tword2id= sorted(word2id.iteritems(), key=lambda d:d[1], reverse = False) #from small to large\n",
    "\twords=[i[0] for i in word2id]\n",
    "\tfor i in range(topicnum):\n",
    "\t\twordlist=lda.get_topic_terms(i,topn=20)\n",
    "\t\tliststring=''\n",
    "\t\tfor n in range(20):\n",
    "\t\t\tId=wordlist[n][0]\n",
    "\t\t\tword=words[Id]\n",
    "\t\t\tliststring=liststring+','+word#+','+unicode(wordlist[n][1])\n",
    "\t\tf4.write('%d%s\\n'%(i,liststring))\n",
    "\tf4.close\n",
    "\treturn corpus,word2id\n",
    "\n",
    "def SubMat(corpus,wordID):\n",
    "\tMat=[] #2 tuple (appear or not,topic)\n",
    "\tn=1\n",
    "\tfor document in corpus:\n",
    "\t\tfor word in document[:-1]:\n",
    "\t\t\tif wordID in word:\n",
    "\t\t\t\tMat.append((1,document[-1]))\n",
    "\t\t\t\tbreak\n",
    "\t\tif n==len(Mat):\n",
    "\t\t\tMat.append((0,document[-1]))\n",
    "\t\tn=n+1\n",
    "\treturn Mat\n",
    "\n",
    "def calEntropy(Mat,topic): #bigger is better\n",
    "\tMI=0\n",
    "\tfor i in range(0,2): #word is present or absent\n",
    "\t\tsublist=[a[1] for a in Mat if a[0] == i] #topic class of docs with/without word appers in\n",
    "\t\tn=sublist.count(topic)\n",
    "\t\tm=len(Mat)-n\n",
    "\t\tk=len(sublist)-n\n",
    "\t\tp1=float(n+1)/(len(Mat)+2) #P(X1,Y) --X1:belong to topic X, X0 not belong,Y:word shows or not\n",
    "\t\tp2=float(n+1)/(len(sublist)+2) #P(X1|Y)\n",
    "\t\tp3=float(m+1)/(len(Mat)+2) #P(X0,Y)\n",
    "\t\tp4=float(k+1)/(len(sublist)+2) #P(X0|Y)\n",
    "\t\tMI=MI+p1*log(p2,2)+p3*log(p4,2)\n",
    "\treturn MI\n",
    "\n",
    "def filterEntropy(Mat,topic): #bigger is better\n",
    "\tMI=0\n",
    "\tfor i in range(0,2): #word is present or absent\n",
    "\t\tsublist=[a[1] for a in Mat if a[0] == i] #topic class of docs with/without word appers in\n",
    "\t\tn=sublist.count(topic)\n",
    "\t\tm=len(Mat)-n\n",
    "\t\tk=len(sublist)-n\n",
    "\t\tp1=float(n+1)/(len(Mat)+2) #P(X1,Y) --X1:belong to topic X, X0 not belong,Y:word shows or not\n",
    "\t\tp2=float(n+1)/(len(sublist)+2) #P(X1|Y)\n",
    "\t\tp3=float(m+1)/(len(Mat)+2) #P(X0,Y)\n",
    "\t\tp4=float(k+1)/(len(sublist)+2) #P(X0|Y)\n",
    "\t\tMI=MI+p1*log(p2,2)+p3*log(p4,2)\n",
    "\tif p2>p4:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "Documents,Stoplist=initiate()\n",
    "BagOfWord(Documents,Stoplist)\n",
    "dictionary,corpus=BagOfWord(Documents,Stoplist)\n",
    "topicnum=5\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=topicnum, update_every=1, chunksize=10000, passes=1)\n",
    "corpus,word2id=output(corpus,dictionary,lda)\n",
    "MI_Matrix=[[0]*topicnum]*len(word2id)\n",
    "MI_Matrix2=[[0]*topicnum]*len(word2id)\n",
    "for n in range(len(word2id)):\n",
    "\tId=word2id[n][1]\n",
    "\tMat=SubMat(corpus,Id)\n",
    "\tfor i in range(topicnum): #iterate topics\n",
    "\t\tMI_Matrix[n][i]=calEntropy(Mat,i)\n",
    "\t\tMI_Matrix2[n][i]=filterEntropy(Mat,i)\n",
    "\tprint 'MI',word2id[n][0],MI_Matrix[n]\n",
    "\tprint 'filter',word2id[n][0],MI_Matrix2[n]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
